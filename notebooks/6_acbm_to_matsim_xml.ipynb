{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import acbm\n",
    "from pam.read import load_travel_diary\n",
    "from pam import write\n",
    "\n",
    "from shapely import wkt, Point\n",
    "\n",
    "from acbm.postprocessing.matsim import (\n",
    "    # add_home_location_to_individuals,\n",
    "    # calculate_percentage_remaining,\n",
    "    # filter_by_pid,\n",
    "    # filter_no_location,\n",
    "    # log_row_count,\n",
    "    get_passengers,\n",
    "    get_pt_subscription,\n",
    "    get_students,\n",
    "    get_hhlIncome,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to save acbm output as matsim xml files using PAM. A number of preprocessing steps need to be done befehand\n",
    "Ideally, some of these should be fixed upstream in the acbm code. Some may be the result of NTS data issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "individuals = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/people.csv\")\n",
    "households = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/households.csv\")\n",
    "activities = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/activities.csv\")\n",
    "legs = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/legs.csv\")\n",
    "legs_geo = pd.read_parquet(acbm.root_path / \"data/processed/activities_pam/legs_with_locations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1709"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individuals.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # rename age_years to age in individuals\n",
    "individuals.rename(columns={\"age_years\": \"age\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc_with_nts = pd.read_parquet(acbm.root_path / \"data/interim/matching/spc_with_nts_trips.parquet\")\n",
    "spc_with_nts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>household</th>\n",
       "      <th>workplace</th>\n",
       "      <th>events</th>\n",
       "      <th>weekday_diaries</th>\n",
       "      <th>weekend_diaries</th>\n",
       "      <th>orig_pid</th>\n",
       "      <th>id_tus_hh</th>\n",
       "      <th>id_tus_p</th>\n",
       "      <th>pid_hs</th>\n",
       "      <th>...</th>\n",
       "      <th>accommodation_type</th>\n",
       "      <th>communal_type</th>\n",
       "      <th>num_rooms</th>\n",
       "      <th>central_heat</th>\n",
       "      <th>tenure</th>\n",
       "      <th>num_cars</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_years</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nssec8_household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 3.6287833784528047e-16, 'concert...</td>\n",
       "      <td>[954, 1037, 1234, 2981, 6290, 9535, 10385, 106...</td>\n",
       "      <td>[955, 1036, 1235, 2980, 6291, 9536, 10384, 106...</td>\n",
       "      <td>E02002330_0001_001</td>\n",
       "      <td>34051017</td>\n",
       "      <td>1</td>\n",
       "      <td>2911721</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 9.903925281880971e-14, 'concert_...</td>\n",
       "      <td>[3435, 6069, 13203, 14704]</td>\n",
       "      <td>[3436, 6068, 13202, 14703]</td>\n",
       "      <td>E02002330_0001_002</td>\n",
       "      <td>21040818</td>\n",
       "      <td>1</td>\n",
       "      <td>2904618</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 1.2791347489984115e-31, 'concert...</td>\n",
       "      <td>[762, 5168, 6201, 8977]</td>\n",
       "      <td>[761, 5169, 6200, 8976]</td>\n",
       "      <td>E02002330_0002_001</td>\n",
       "      <td>11131017</td>\n",
       "      <td>1</td>\n",
       "      <td>2902311</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 7.754311082130982e-10, 'concert_...</td>\n",
       "      <td>[1580, 5417, 5956, 12901]</td>\n",
       "      <td>[1581, 5416, 5957, 12900]</td>\n",
       "      <td>E02002330_0003_001</td>\n",
       "      <td>15020311</td>\n",
       "      <td>1</td>\n",
       "      <td>2911131</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>508.0</td>\n",
       "      <td>{'concert_f': 2.1388457227544677e-08, 'concert...</td>\n",
       "      <td>[318, 3145, 10496, 12819, 13943]</td>\n",
       "      <td>[319, 3144, 10495, 12818, 13942]</td>\n",
       "      <td>E02002330_0003_002</td>\n",
       "      <td>20090607</td>\n",
       "      <td>1</td>\n",
       "      <td>2909582</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794634</th>\n",
       "      <td>794634</td>\n",
       "      <td>334848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 0.030085181817412376, 'concert_f...</td>\n",
       "      <td>[253, 904, 960, 1258, 1666, 1827, 2990, 3158, ...</td>\n",
       "      <td>[252, 903, 961, 1259, 1667, 1826, 2991, 3159, ...</td>\n",
       "      <td>E02006876_3658_001</td>\n",
       "      <td>15171109</td>\n",
       "      <td>2</td>\n",
       "      <td>2910202</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794635</th>\n",
       "      <td>794635</td>\n",
       "      <td>334849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 5.36953439222998e-06, 'concert_f...</td>\n",
       "      <td>[1611, 2074, 3331, 3973, 5305, 7241, 9500, 10413]</td>\n",
       "      <td>[1610, 2075, 3330, 3974, 5304, 7240, 9499, 10412]</td>\n",
       "      <td>E02006876_3659_001</td>\n",
       "      <td>12080913</td>\n",
       "      <td>2</td>\n",
       "      <td>2903691</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794636</th>\n",
       "      <td>794636</td>\n",
       "      <td>334850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 9.81540181321244e-28, 'concert_f...</td>\n",
       "      <td>[1768, 1833, 2004, 3538, 5690, 5693, 9177, 112...</td>\n",
       "      <td>[1767, 1832, 2005, 3537, 5691, 5692, 9176, 112...</td>\n",
       "      <td>E02006876_3660_001</td>\n",
       "      <td>15291209</td>\n",
       "      <td>1</td>\n",
       "      <td>2905917</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794637</th>\n",
       "      <td>794637</td>\n",
       "      <td>334851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'concert_f': 0.05208142101764679, 'concert_fs...</td>\n",
       "      <td>[2173, 14404, 15340, 16376]</td>\n",
       "      <td>[2174, 14403, 15339, 16377]</td>\n",
       "      <td>E02006876_3661_001</td>\n",
       "      <td>20280416</td>\n",
       "      <td>1</td>\n",
       "      <td>2907191</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794638</th>\n",
       "      <td>794638</td>\n",
       "      <td>334851</td>\n",
       "      <td>31329.0</td>\n",
       "      <td>{'concert_f': 5.096165657043457, 'concert_fs':...</td>\n",
       "      <td>[3393, 3729, 4300, 5119, 10052, 15867, 15970]</td>\n",
       "      <td>[3392, 3730, 4299, 5118, 10051, 15866, 15969]</td>\n",
       "      <td>E02006876_3661_002</td>\n",
       "      <td>13031119</td>\n",
       "      <td>1</td>\n",
       "      <td>2907075</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>794639 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  household  workplace  \\\n",
       "0            0          0        NaN   \n",
       "1            1          0        NaN   \n",
       "2            2          1        NaN   \n",
       "3            3          2        NaN   \n",
       "4            4          2      508.0   \n",
       "...        ...        ...        ...   \n",
       "794634  794634     334848        NaN   \n",
       "794635  794635     334849        NaN   \n",
       "794636  794636     334850        NaN   \n",
       "794637  794637     334851        NaN   \n",
       "794638  794638     334851    31329.0   \n",
       "\n",
       "                                                   events  \\\n",
       "0       {'concert_f': 3.6287833784528047e-16, 'concert...   \n",
       "1       {'concert_f': 9.903925281880971e-14, 'concert_...   \n",
       "2       {'concert_f': 1.2791347489984115e-31, 'concert...   \n",
       "3       {'concert_f': 7.754311082130982e-10, 'concert_...   \n",
       "4       {'concert_f': 2.1388457227544677e-08, 'concert...   \n",
       "...                                                   ...   \n",
       "794634  {'concert_f': 0.030085181817412376, 'concert_f...   \n",
       "794635  {'concert_f': 5.36953439222998e-06, 'concert_f...   \n",
       "794636  {'concert_f': 9.81540181321244e-28, 'concert_f...   \n",
       "794637  {'concert_f': 0.05208142101764679, 'concert_fs...   \n",
       "794638  {'concert_f': 5.096165657043457, 'concert_fs':...   \n",
       "\n",
       "                                          weekday_diaries  \\\n",
       "0       [954, 1037, 1234, 2981, 6290, 9535, 10385, 106...   \n",
       "1                              [3435, 6069, 13203, 14704]   \n",
       "2                                 [762, 5168, 6201, 8977]   \n",
       "3                               [1580, 5417, 5956, 12901]   \n",
       "4                        [318, 3145, 10496, 12819, 13943]   \n",
       "...                                                   ...   \n",
       "794634  [253, 904, 960, 1258, 1666, 1827, 2990, 3158, ...   \n",
       "794635  [1611, 2074, 3331, 3973, 5305, 7241, 9500, 10413]   \n",
       "794636  [1768, 1833, 2004, 3538, 5690, 5693, 9177, 112...   \n",
       "794637                        [2173, 14404, 15340, 16376]   \n",
       "794638      [3393, 3729, 4300, 5119, 10052, 15867, 15970]   \n",
       "\n",
       "                                          weekend_diaries            orig_pid  \\\n",
       "0       [955, 1036, 1235, 2980, 6291, 9536, 10384, 106...  E02002330_0001_001   \n",
       "1                              [3436, 6068, 13202, 14703]  E02002330_0001_002   \n",
       "2                                 [761, 5169, 6200, 8976]  E02002330_0002_001   \n",
       "3                               [1581, 5416, 5957, 12900]  E02002330_0003_001   \n",
       "4                        [319, 3144, 10495, 12818, 13942]  E02002330_0003_002   \n",
       "...                                                   ...                 ...   \n",
       "794634  [252, 903, 961, 1259, 1667, 1826, 2991, 3159, ...  E02006876_3658_001   \n",
       "794635  [1610, 2075, 3330, 3974, 5304, 7240, 9499, 10412]  E02006876_3659_001   \n",
       "794636  [1767, 1832, 2005, 3537, 5691, 5692, 9176, 112...  E02006876_3660_001   \n",
       "794637                        [2174, 14403, 15339, 16377]  E02006876_3661_001   \n",
       "794638      [3392, 3730, 4299, 5118, 10051, 15866, 15969]  E02006876_3661_002   \n",
       "\n",
       "        id_tus_hh  id_tus_p   pid_hs  ... accommodation_type communal_type  \\\n",
       "0        34051017         1  2911721  ...                2.0           NaN   \n",
       "1        21040818         1  2904618  ...                2.0           NaN   \n",
       "2        11131017         1  2902311  ...                1.0           NaN   \n",
       "3        15020311         1  2911131  ...                1.0           NaN   \n",
       "4        20090607         1  2909582  ...                1.0           NaN   \n",
       "...           ...       ...      ...  ...                ...           ...   \n",
       "794634   15171109         2  2910202  ...                3.0           NaN   \n",
       "794635   12080913         2  2903691  ...                2.0           NaN   \n",
       "794636   15291209         1  2905917  ...                4.0           NaN   \n",
       "794637   20280416         1  2907191  ...                4.0           NaN   \n",
       "794638   13031119         1  2907075  ...                4.0           NaN   \n",
       "\n",
       "       num_rooms  central_heat  tenure  num_cars  sex  age_years  ethnicity  \\\n",
       "0            6.0          True     2.0         2    1         68          1   \n",
       "1            6.0          True     2.0         2    2         65          1   \n",
       "2            5.0          True     1.0         2    1         86          1   \n",
       "3            6.0          True     2.0         1    1         58          1   \n",
       "4            6.0          True     2.0         1    2         56          1   \n",
       "...          ...           ...     ...       ...  ...        ...        ...   \n",
       "794634       6.0          True     1.0         1    1         39          1   \n",
       "794635       5.0         False     1.0         0    2         51          1   \n",
       "794636       2.0          True     5.0         0    1         82          1   \n",
       "794637       3.0          True     5.0         0    1         38          1   \n",
       "794638       3.0          True     5.0         0    2         22          2   \n",
       "\n",
       "        nssec8_household  \n",
       "0                    1.0  \n",
       "1                    6.0  \n",
       "2                    2.0  \n",
       "3                    4.0  \n",
       "4                    1.0  \n",
       "...                  ...  \n",
       "794634               3.0  \n",
       "794635               7.0  \n",
       "794636               NaN  \n",
       "794637               2.0  \n",
       "794638               2.0  \n",
       "\n",
       "[794639 rows x 38 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spc = pd.read_parquet(acbm.root_path / \"data/external/spc_output/leeds_people_hh.parquet\")\n",
    "spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sex column to individuals\n",
    "\n",
    "spc = pd.read_parquet(acbm.root_path / \"data/external/spc_output/leeds_people_hh.parquet\", \n",
    "                      columns=[\"id\", \"household\", \"age_years\", \"sex\", \"salary_yearly\"])\n",
    "spc.head(5)\n",
    "\n",
    "# change spc[\"sex\"] column: 1 = male, 2 = female\n",
    "\n",
    "spc[\"sex\"] = spc[\"sex\"].map({1:'male',\n",
    "                             2: 'female'})\n",
    "\n",
    "spc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = individuals.merge(spc[[\"id\", \"sex\"]], left_on=\"pid\", right_on=\"id\", how=\"left\")\n",
    "individuals = individuals.drop(columns=\"id\")\n",
    "individuals.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = get_passengers(\n",
    "    legs = legs, \n",
    "    individuals = individuals, \n",
    "    modes = ['car_passenger', 'taxi'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = get_pt_subscription(individuals = individuals, age_threshold = 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = get_students(\n",
    "    individuals = individuals,\n",
    "    activities = activities,\n",
    "    age_base_threshold = 16,\n",
    "    #age_upper_threshold = 30,\n",
    "    activity = 'education')\n",
    "\n",
    "individuals.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of x by age and isStudent \n",
    "individuals.groupby(['age', 'isStudent']).size().unstack().plot(kind='bar', stacked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = get_hhlIncome(\n",
    "        individuals = individuals,\n",
    "        individuals_with_salary = spc,\n",
    "        pension_age = 66,\n",
    "        pension = 13000)\n",
    "\n",
    "individuals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be removing some rows in each planning operation. This function helps keep a \n",
    "# record of the number of rows in each table after each operation.\n",
    "\n",
    "row_counts = []\n",
    "\n",
    "# # Function to log row counts\n",
    "def log_row_count(df, name, operation):\n",
    "    row_counts.append((operation, name, len(df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_row_count(individuals, \"individuals\", \"0_initial\")\n",
    "log_row_count(households, \"households\", \"0_initial\")\n",
    "log_row_count(activities, \"activities\", \"0_initial\")\n",
    "log_row_count(legs, \"legs\", \"0_initial\")\n",
    "log_row_count(legs_geo, \"legs_geo\", \"0_initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove people that don't exist across all datasets\n",
    "\n",
    "When writing to matsim using pam, we get an error when a pid exists in one dataset but not in the other. We will remove these people from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_pid(individuals, activities, legs, legs_geo, households):\n",
    "    \"\"\"\n",
    "    Filter the input DataFrames to include only include people (pids) that exist in all\n",
    "    dfs \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individuals: pd.DataFrame\n",
    "        Individuals DataFrame.\n",
    "    activities: pd.DataFrame\n",
    "        Activities DataFrame.\n",
    "    legs: pd.DataFrame: \n",
    "        Legs DataFrame.\n",
    "    legs_geo: pd.DataFrame\n",
    "        Legs with geo DataFrame.\n",
    "    households: pd.DataFrame\n",
    "        Households DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the filtered DataFrames (individuals, activities, legs, legs_geo, households).\n",
    "    \"\"\"\n",
    "    # Identify common pids\n",
    "    common_pids = set(individuals[\"pid\"]).intersection(activities[\"pid\"]).intersection(legs[\"pid\"]).intersection(legs_geo[\"pid\"])\n",
    "\n",
    "    # Filter Individual Level DataFrames\n",
    "    individuals = individuals[individuals[\"pid\"].isin(common_pids)]\n",
    "    activities = activities[activities[\"pid\"].isin(common_pids)]\n",
    "    legs = legs[legs[\"pid\"].isin(common_pids)]\n",
    "    legs_geo = legs_geo[legs_geo[\"pid\"].isin(common_pids)]\n",
    "\n",
    "    # Filter Household Level DataFrame\n",
    "    households = households[households[\"hid\"].isin(individuals[\"hid\"])]\n",
    "\n",
    "    return individuals, activities, legs, legs_geo, households\n",
    "\n",
    "# Apply\n",
    "individuals, activities, legs, legs_geo, households = filter_by_pid(individuals, activities, legs, legs_geo, households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_row_count(individuals, \"individuals\", \"1_filter_by_pid\")\n",
    "log_row_count(households, \"households\", \"1_filter_by_pid\")\n",
    "log_row_count(activities, \"activities\", \"1_filter_by_pid\")\n",
    "log_row_count(legs, \"legs\", \"1_filter_by_pid\")\n",
    "log_row_count(legs_geo, \"legs_geo\", \"1_filter_by_pid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(row_counts, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all rows where start_location_geometry_wkt is null\n",
    "legs_geo[legs_geo['start_location_geometry_wkt'].isnull()]\n",
    "\n",
    "# all rows where end_location_geometry_wkt is null\n",
    "#legs_geo[legs_geo['end_location_geometry_wkt'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns for PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rename in 3.3_assign_facility_all script\n",
    "# rename start_location_geometry_wkt and end_location_geometry_wkt to start_loc and end_loc\n",
    "legs_geo.rename(columns={\"start_location_geometry_wkt\": \"start_loc\", \"end_location_geometry_wkt\": \"end_loc\"}, inplace=True)\n",
    "legs_geo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove people with missing locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_no_location(individuals, households, activities, legs, legs_geo):\n",
    "    \"\"\"\n",
    "    Cleans the provided DataFrames by removing rows without location data. Gets all pids\n",
    "    that have at least one row with missing location data, and removes all rows with \n",
    "    these pids. pids are geneerated from two sources: \n",
    "       - legs_geo with missing start_loc or end_loc\n",
    "       - individuals with missing hzone \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individuals : pd.DataFrame\n",
    "        DataFrame containing individual data.\n",
    "    households : pd.DataFrame\n",
    "        DataFrame containing household data.\n",
    "    activities : pd.DataFrame\n",
    "        DataFrame containing activity data.\n",
    "    legs : pd.DataFrame\n",
    "        DataFrame containing legs data.\n",
    "    legs_geo : pd.DataFrame\n",
    "        DataFrame containing legs with geographic data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the cleaned DataFrames (individuals_cleaned, households_cleaned, activities_cleaned, legs_cleaned, legs_geo_cleaned).\n",
    "    \"\"\"\n",
    "    # Identify rows in legs_geo where start_loc or end_loc are null\n",
    "    invalid_rows_legs_geo = legs_geo[legs_geo[\"start_loc\"].isnull() | legs_geo[\"end_loc\"].isnull()]\n",
    "\n",
    "    # Extract the pid values associated with these rows\n",
    "    invalid_pids_legs_geo = invalid_rows_legs_geo[\"pid\"].unique()\n",
    "\n",
    "    # Identify rows in individuals where hzone is null\n",
    "    invalid_rows_individuals = individuals[individuals[\"hzone\"].isnull()]\n",
    "\n",
    "    # Extract the pid values associated with these rows\n",
    "    invalid_pids_individuals = invalid_rows_individuals[\"pid\"].unique()\n",
    "\n",
    "    # Combine the invalid pid values from both sources\n",
    "    invalid_pids = set(invalid_pids_legs_geo).union(set(invalid_pids_individuals))\n",
    "\n",
    "    # Remove rows with these pids from all DataFrames\n",
    "    individuals_cleaned = individuals[~individuals[\"pid\"].isin(invalid_pids)]\n",
    "    activities_cleaned = activities[~activities[\"pid\"].isin(invalid_pids)]\n",
    "    legs_cleaned = legs[~legs[\"pid\"].isin(invalid_pids)]\n",
    "    legs_geo_cleaned = legs_geo[~legs_geo[\"pid\"].isin(invalid_pids)]\n",
    "\n",
    "    # Extract remaining hid values from individuals_cleaned\n",
    "    remaining_hids = individuals_cleaned[\"hid\"].unique()\n",
    "\n",
    "    # Filter households_cleaned to only include rows with hid values in remaining_hids\n",
    "    households_cleaned = households[households[\"hid\"].isin(remaining_hids)]\n",
    "\n",
    "    return individuals_cleaned, households_cleaned, activities_cleaned, legs_cleaned, legs_geo_cleaned\n",
    "\n",
    "# Apply\n",
    "individuals, households, activities, legs, legs_geo = filter_no_location(individuals, \n",
    "                                                                         households, \n",
    "                                                                         activities, \n",
    "                                                                         legs, \n",
    "                                                                         legs_geo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_row_count(individuals, \"individuals\", \"2_filter_no_location\")\n",
    "log_row_count(households, \"households\", \"2_filter_no_location\")\n",
    "log_row_count(activities, \"activities\", \"2_filter_no_location\")\n",
    "log_row_count(legs, \"legs\", \"2_filter_no_location\")\n",
    "log_row_count(legs_geo, \"legs_geo\", \"2_filter_no_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_remaining(row_counts):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of rows remaining for each DataFrame based on the initial counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row_counts : list of tuples\n",
    "        List of tuples containing stage, DataFrame names, and their row counts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        List of tuples containing stage, DataFrame names, and their percentage of rows remaining.\n",
    "    \"\"\"\n",
    "    # Extract initial counts\n",
    "    initial_counts = {df_name: count for stage, df_name, count in row_counts if stage == '0_initial'}\n",
    "\n",
    "    # Calculate percentage remaining\n",
    "    percentage_remaining = []\n",
    "    for stage, df_name, count in row_counts:\n",
    "        if df_name in initial_counts:\n",
    "            initial_count = initial_counts[df_name]\n",
    "            percentage = round((count / initial_count) * 100, 1)\n",
    "            percentage_remaining.append((stage, df_name, count, percentage))\n",
    "\n",
    "    # Sort by df_name\n",
    "    percentage_remaining.sort(key=lambda x: x[1])\n",
    "\n",
    "    return percentage_remaining\n",
    "\n",
    "\n",
    "percentages = calculate_percentage_remaining(row_counts)\n",
    "\n",
    "# Print the percentages\n",
    "for stage, df_name, count, percentage in percentages:\n",
    "    print(f\"{stage} - {df_name} - {count} rows: {percentage:.1f}% rows remaining\")\n",
    "\n",
    "# # Log the percentages\n",
    "# for stage, df_name, count, percentage in percentages:\n",
    "#     logging.info(f\"{stage} - {df_name} - {count} rows: {percentage:.1f}% rows remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to Point if not already a Point\n",
    "def convert_to_point(value):\n",
    "    if isinstance(value, Point):\n",
    "        return value\n",
    "    return wkt.loads(value)\n",
    "\n",
    "# Convert start_loc and end_loc to shapely point objects\n",
    "legs_geo[\"start_loc\"] = legs_geo[\"start_loc\"].apply(convert_to_point)\n",
    "legs_geo[\"end_loc\"] = legs_geo[\"end_loc\"].apply(convert_to_point)\n",
    "\n",
    "# Verify the type of the first element in the 'start_loc' column\n",
    "print(type(legs_geo['start_loc'].iloc[0]))  # Should be <class 'shapely.geometry.point.Point'>\n",
    "\n",
    "# Convert to GeoDataFrame with start_loc as the active geometry\n",
    "legs_geo = gpd.GeoDataFrame(legs_geo, geometry='start_loc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add home location to individuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_home_location_to_individuals(legs_geo, individuals):\n",
    "    \"\"\"\n",
    "    Adds home location to individuals dataframe. Location is obtained \n",
    "    from legs_geo (rows with orign activity = home) \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    legs_geo : pd.DataFrame\n",
    "        DataFrame containing legs with geographic data.\n",
    "    individuals : pd.DataFrame\n",
    "        DataFrame containing individual data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The modified individuals DataFrame with location information.\n",
    "    \"\"\"\n",
    "    # Filter by origin activity = home\n",
    "    legs_geo_home = legs_geo[legs_geo[\"origin activity\"] == \"home\"]\n",
    "    \n",
    "    # Get one row for each hid group\n",
    "    legs_geo_home = legs_geo_home.groupby(\"hid\").first().reset_index()\n",
    "    \n",
    "    # Keep only the columns we need: hid and start_location\n",
    "    legs_geo_home = legs_geo_home[[\"hid\", \"start_loc\"]]\n",
    "    \n",
    "    # Rename start_loc to loc\n",
    "    legs_geo_home.rename(columns={\"start_loc\": \"loc\"}, inplace=True)\n",
    "    \n",
    "    # Merge legs_geo_home with individuals\n",
    "    individuals_geo = individuals.copy()\n",
    "    individuals_geo = individuals_geo.merge(legs_geo_home, on=\"hid\")\n",
    "    \n",
    "    # Remove rows with missing loc\n",
    "    individuals_geo = individuals_geo[individuals_geo[\"loc\"].notnull()]\n",
    "    \n",
    "    return individuals_geo\n",
    "\n",
    "# Apply\n",
    "individuals_geo = add_home_location_to_individuals(legs_geo, individuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Car Ownership\n",
    "\n",
    "TODO: get num_cars per household from spc_with_nts\n",
    "\n",
    "this can then be passed on using hhs_attributes in pam.load_travel_diary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = load_travel_diary(\n",
    "        trips=legs_geo,\n",
    "        persons_attributes=individuals,\n",
    "        tour_based=False,\n",
    "        include_loc=True,\n",
    "        sort_by_seq=True,\n",
    "        # hhs_attributes = None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population[89][200].print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jitter the plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from pam.samplers.time import apply_jitter_to_plan\n",
    "\n",
    "\n",
    "for hid, pid, person in population.people():\n",
    "    apply_jitter_to_plan(\n",
    "        person.plan,\n",
    "        jitter=timedelta(minutes=30),\n",
    "        min_duration=timedelta(minutes=10)\n",
    "    )\n",
    "    # crop to 24-hours\n",
    "    person.plan.crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population[89][200].print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the population to matsim xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.write_matsim_population_v6(\n",
    "    population=population,\n",
    "    path= acbm.root_path / \"data/processed/activities_pam/plans.xml\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acbm-7iKwKWLy-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
