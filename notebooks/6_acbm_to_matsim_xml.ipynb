{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import acbm\n",
    "from pam.read import load_travel_diary\n",
    "from pam import write\n",
    "\n",
    "from shapely import wkt, Point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to save acbm output as matsim xml files using PAM. A number of preprocessing steps need to be done befehand\n",
    "Ideally, some of these should be fixed upstream in the acbm code. Some may be the result of NTS data issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "individuals = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/people.csv\")\n",
    "households = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/households.csv\")\n",
    "activities = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/activities.csv\")\n",
    "legs = pd.read_csv(acbm.root_path / \"data/processed/activities_pam/legs.csv\")\n",
    "legs_geo = pd.read_parquet(acbm.root_path / \"data/processed/activities_pam/legs_with_locations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legs_geo.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be removing some rows in each planning operation. This function helps keep a \n",
    "# record of the number of rows in each table after each operation.\n",
    "\n",
    "row_counts = []\n",
    "\n",
    "# # Function to log row counts\n",
    "def log_row_count(df, name, operation):\n",
    "    row_counts.append((operation, name, len(df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_row_count(individuals, \"individuals\", \"0_initial\")\n",
    "log_row_count(households, \"households\", \"0_initial\")\n",
    "log_row_count(activities, \"activities\", \"0_initial\")\n",
    "log_row_count(legs, \"legs\", \"0_initial\")\n",
    "log_row_count(legs_geo, \"legs_geo\", \"0_initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove people that don't exist across all datasets\n",
    "\n",
    "When writing to matsim using pam, we get an error when a pid exists in one dataset but not in the other. We will remove these people from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_pid(individuals, activities, legs, legs_geo, households):\n",
    "    \"\"\"\n",
    "    Filter the input DataFrames to include only include people (pids) that exist in all\n",
    "    dfs \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individuals: pd.DataFrame\n",
    "        Individuals DataFrame.\n",
    "    activities: pd.DataFrame\n",
    "        Activities DataFrame.\n",
    "    legs: pd.DataFrame: \n",
    "        Legs DataFrame.\n",
    "    legs_geo: pd.DataFrame\n",
    "        Legs with geo DataFrame.\n",
    "    households: pd.DataFrame\n",
    "        Households DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the filtered DataFrames (individuals, activities, legs, legs_geo, households).\n",
    "    \"\"\"\n",
    "    # Identify common pids\n",
    "    common_pids = set(individuals[\"pid\"]).intersection(activities[\"pid\"]).intersection(legs[\"pid\"]).intersection(legs_geo[\"pid\"])\n",
    "\n",
    "    # Filter Individual Level DataFrames\n",
    "    individuals = individuals[individuals[\"pid\"].isin(common_pids)]\n",
    "    activities = activities[activities[\"pid\"].isin(common_pids)]\n",
    "    legs = legs[legs[\"pid\"].isin(common_pids)]\n",
    "    legs_geo = legs_geo[legs_geo[\"pid\"].isin(common_pids)]\n",
    "\n",
    "    # Filter Household Level DataFrame\n",
    "    households = households[households[\"hid\"].isin(individuals[\"hid\"])]\n",
    "\n",
    "    return individuals, activities, legs, legs_geo, households\n",
    "\n",
    "# Apply\n",
    "individuals, activities, legs, legs_geo, households = filter_by_pid(individuals, activities, legs, legs_geo, households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_row_count(individuals, \"individuals\", \"1_filter_by_pid\")\n",
    "log_row_count(households, \"households\", \"1_filter_by_pid\")\n",
    "log_row_count(activities, \"activities\", \"1_filter_by_pid\")\n",
    "log_row_count(legs, \"legs\", \"1_filter_by_pid\")\n",
    "log_row_count(legs_geo, \"legs_geo\", \"1_filter_by_pid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(row_counts, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all rows where start_location_geometry_wkt is null\n",
    "legs_geo[legs_geo['start_location_geometry_wkt'].isnull()]\n",
    "\n",
    "# all rows where end_location_geometry_wkt is null\n",
    "#legs_geo[legs_geo['end_location_geometry_wkt'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns for PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rename in 3.3_assign_facility_all script\n",
    "# rename start_location_geometry_wkt and end_location_geometry_wkt to start_loc and end_loc\n",
    "legs_geo.rename(columns={\"start_location_geometry_wkt\": \"start_loc\", \"end_location_geometry_wkt\": \"end_loc\"}, inplace=True)\n",
    "legs_geo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove people with missing locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_no_location(individuals, households, activities, legs, legs_geo):\n",
    "    \"\"\"\n",
    "    Cleans the provided DataFrames by removing rows without location data. Gets all pids\n",
    "    that have at least one row with missing location data, and removes all rows with \n",
    "    these pids. pids are geneerated from two sources: \n",
    "       - legs_geo with missing start_loc or end_loc\n",
    "       - individuals with missing hzone \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individuals : pd.DataFrame\n",
    "        DataFrame containing individual data.\n",
    "    households : pd.DataFrame\n",
    "        DataFrame containing household data.\n",
    "    activities : pd.DataFrame\n",
    "        DataFrame containing activity data.\n",
    "    legs : pd.DataFrame\n",
    "        DataFrame containing legs data.\n",
    "    legs_geo : pd.DataFrame\n",
    "        DataFrame containing legs with geographic data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the cleaned DataFrames (individuals_cleaned, households_cleaned, activities_cleaned, legs_cleaned, legs_geo_cleaned).\n",
    "    \"\"\"\n",
    "    # Identify rows in legs_geo where start_loc or end_loc are null\n",
    "    invalid_rows_legs_geo = legs_geo[legs_geo[\"start_loc\"].isnull() | legs_geo[\"end_loc\"].isnull()]\n",
    "\n",
    "    # Extract the pid values associated with these rows\n",
    "    invalid_pids_legs_geo = invalid_rows_legs_geo[\"pid\"].unique()\n",
    "\n",
    "    # Identify rows in individuals where hzone is null\n",
    "    invalid_rows_individuals = individuals[individuals[\"hzone\"].isnull()]\n",
    "\n",
    "    # Extract the pid values associated with these rows\n",
    "    invalid_pids_individuals = invalid_rows_individuals[\"pid\"].unique()\n",
    "\n",
    "    # Combine the invalid pid values from both sources\n",
    "    invalid_pids = set(invalid_pids_legs_geo).union(set(invalid_pids_individuals))\n",
    "\n",
    "    # Remove rows with these pids from all DataFrames\n",
    "    individuals_cleaned = individuals[~individuals[\"pid\"].isin(invalid_pids)]\n",
    "    activities_cleaned = activities[~activities[\"pid\"].isin(invalid_pids)]\n",
    "    legs_cleaned = legs[~legs[\"pid\"].isin(invalid_pids)]\n",
    "    legs_geo_cleaned = legs_geo[~legs_geo[\"pid\"].isin(invalid_pids)]\n",
    "\n",
    "    # Extract remaining hid values from individuals_cleaned\n",
    "    remaining_hids = individuals_cleaned[\"hid\"].unique()\n",
    "\n",
    "    # Filter households_cleaned to only include rows with hid values in remaining_hids\n",
    "    households_cleaned = households[households[\"hid\"].isin(remaining_hids)]\n",
    "\n",
    "    return individuals_cleaned, households_cleaned, activities_cleaned, legs_cleaned, legs_geo_cleaned\n",
    "\n",
    "# Apply\n",
    "individuals, households, activities, legs, legs_geo = filter_no_location(individuals, \n",
    "                                                                         households, \n",
    "                                                                         activities, \n",
    "                                                                         legs, \n",
    "                                                                         legs_geo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_row_count(individuals, \"individuals\", \"2_filter_no_location\")\n",
    "log_row_count(households, \"households\", \"2_filter_no_location\")\n",
    "log_row_count(activities, \"activities\", \"2_filter_no_location\")\n",
    "log_row_count(legs, \"legs\", \"2_filter_no_location\")\n",
    "log_row_count(legs_geo, \"legs_geo\", \"2_filter_no_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_remaining(row_counts):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of rows remaining for each DataFrame based on the initial counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row_counts : list of tuples\n",
    "        List of tuples containing stage, DataFrame names, and their row counts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        List of tuples containing stage, DataFrame names, and their percentage of rows remaining.\n",
    "    \"\"\"\n",
    "    # Extract initial counts\n",
    "    initial_counts = {df_name: count for stage, df_name, count in row_counts if stage == '0_initial'}\n",
    "\n",
    "    # Calculate percentage remaining\n",
    "    percentage_remaining = []\n",
    "    for stage, df_name, count in row_counts:\n",
    "        if df_name in initial_counts:\n",
    "            initial_count = initial_counts[df_name]\n",
    "            percentage = round((count / initial_count) * 100, 1)\n",
    "            percentage_remaining.append((stage, df_name, count, percentage))\n",
    "\n",
    "    # Sort by df_name\n",
    "    percentage_remaining.sort(key=lambda x: x[1])\n",
    "\n",
    "    return percentage_remaining\n",
    "\n",
    "\n",
    "percentages = calculate_percentage_remaining(row_counts)\n",
    "\n",
    "# Print the percentages\n",
    "for stage, df_name, count, percentage in percentages:\n",
    "    print(f\"{stage} - {df_name} - {count} rows: {percentage:.1f}% rows remaining\")\n",
    "\n",
    "# # Log the percentages\n",
    "# for stage, df_name, count, percentage in percentages:\n",
    "#     logging.info(f\"{stage} - {df_name} - {count} rows: {percentage:.1f}% rows remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to Point if not already a Point\n",
    "def convert_to_point(value):\n",
    "    if isinstance(value, Point):\n",
    "        return value\n",
    "    return wkt.loads(value)\n",
    "\n",
    "# Convert start_loc and end_loc to shapely point objects\n",
    "legs_geo[\"start_loc\"] = legs_geo[\"start_loc\"].apply(convert_to_point)\n",
    "legs_geo[\"end_loc\"] = legs_geo[\"end_loc\"].apply(convert_to_point)\n",
    "\n",
    "# Verify the type of the first element in the 'start_loc' column\n",
    "print(type(legs_geo['start_loc'].iloc[0]))  # Should be <class 'shapely.geometry.point.Point'>\n",
    "\n",
    "# Convert to GeoDataFrame with start_loc as the active geometry\n",
    "legs_geo = gpd.GeoDataFrame(legs_geo, geometry='start_loc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add home location to individuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_home_location_to_individuals(legs_geo, individuals):\n",
    "    \"\"\"\n",
    "    Adds home location to individuals dataframe. Location is obtained \n",
    "    from legs_geo (rows with orign activity = home) \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    legs_geo : pd.DataFrame\n",
    "        DataFrame containing legs with geographic data.\n",
    "    individuals : pd.DataFrame\n",
    "        DataFrame containing individual data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The modified individuals DataFrame with location information.\n",
    "    \"\"\"\n",
    "    # Filter by origin activity = home\n",
    "    legs_geo_home = legs_geo[legs_geo[\"origin activity\"] == \"home\"]\n",
    "    \n",
    "    # Get one row for each hid group\n",
    "    legs_geo_home = legs_geo_home.groupby(\"hid\").first().reset_index()\n",
    "    \n",
    "    # Keep only the columns we need: hid and start_location\n",
    "    legs_geo_home = legs_geo_home[[\"hid\", \"start_loc\"]]\n",
    "    \n",
    "    # Rename start_loc to loc\n",
    "    legs_geo_home.rename(columns={\"start_loc\": \"loc\"}, inplace=True)\n",
    "    \n",
    "    # Merge legs_geo_home with individuals\n",
    "    individuals_geo = individuals.copy()\n",
    "    individuals_geo = individuals_geo.merge(legs_geo_home, on=\"hid\")\n",
    "    \n",
    "    # Remove rows with missing loc\n",
    "    individuals_geo = individuals_geo[individuals_geo[\"loc\"].notnull()]\n",
    "    \n",
    "    return individuals_geo\n",
    "\n",
    "# Apply\n",
    "individuals_geo = add_home_location_to_individuals(legs_geo, individuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Car Ownership\n",
    "\n",
    "TODO: get num_cars per household from spc_with_nts\n",
    "\n",
    "this can then be passed on using hhs_attributes in pam.load_travel_diary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = load_travel_diary(\n",
    "        trips=legs_geo,\n",
    "        persons_attributes=individuals,\n",
    "        tour_based=False,\n",
    "        include_loc=True,\n",
    "        sort_by_seq=True,\n",
    "        # hhs_attributes = None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population[89][200].print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jitter the plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from pam.samplers.time import apply_jitter_to_plan\n",
    "\n",
    "\n",
    "for hid, pid, person in population.people():\n",
    "    apply_jitter_to_plan(\n",
    "        person.plan,\n",
    "        jitter=timedelta(minutes=30),\n",
    "        min_duration=timedelta(minutes=10)\n",
    "    )\n",
    "    # crop to 24-hours\n",
    "    person.plan.crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population[89][200].print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the population to matsim xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.write_matsim_population_v6(\n",
    "    population=population,\n",
    "    path= acbm.root_path / \"data/processed/activities_pam/plans.xml\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acbm-7iKwKWLy-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
