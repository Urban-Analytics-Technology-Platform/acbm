{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out how to use PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import acbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pam\n",
    "from pam import read, write\n",
    "from pam.activity import Activity, Leg, Plan\n",
    "from pam.location import Location\n",
    "from pam.planner.choice_location import DiscretionaryTripOD, DiscretionaryTrips\n",
    "from pam.planner.od import ODFactory, ODMatrix\n",
    "from pam.planner.utils_planner import get_trip_chains_either_anchor\n",
    "from pam.plot.stats import plot_activity_times, plot_leg_times\n",
    "from pam.utils import minutes_to_datetime as mtdt\n",
    "from pam.variables import END_OF_DAY\n",
    "from prettytable import PrettyTable\n",
    "from shapely.geometry import Point\n",
    "from libpysal.weights import Queen\n",
    "\n",
    "\n",
    "from acbm.preprocessing import nts_filter_by_year, add_location\n",
    "from acbm.assigning.primary_select import select_facility\n",
    "#from acbm.logger_config import assigning_secondary_locations_logger as logger\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains = pd.read_parquet(\n",
    "    acbm.root_path / \"data/interim/matching/spc_with_nts_trips.parquet\"\n",
    ")\n",
    "activity_chains = activity_chains[activity_chains[\"TravDay\"] == 3]  # Wednesday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all people who don't start their day at home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add OA21CD to the data\n",
    "\n",
    "We will use it to select home locations using select_facility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_clause = \"MSOA21NM LIKE '%Leeds%'\"\n",
    "\n",
    "boundaries = gpd.read_file(\n",
    "    acbm.root_path / \"data/external/boundaries/oa_england.geojson\", where=where_clause\n",
    ")\n",
    "\n",
    "# convert boundaries to 4326\n",
    "boundaries = boundaries.to_crs(epsg=4326)\n",
    "\n",
    "\n",
    "# --- Assign activity home locations to boundaries zoning system\n",
    "\n",
    "# Convert location column in activity_chains to spatial column\n",
    "centroid_layer = pd.read_csv(\n",
    "    acbm.root_path / \"data/external/centroids/Output_Areas_Dec_2011_PWC_2022.csv\"\n",
    ")\n",
    "activity_chains = add_location(\n",
    "    activity_chains, \"EPSG:27700\", \"EPSG:4326\", centroid_layer, \"OA11CD\", \"OA11CD\"\n",
    ")\n",
    "\n",
    "# Convert the DataFrame into a GeoDataFrame, and assign a coordinate reference system (CRS)\n",
    "activity_chains = gpd.GeoDataFrame(activity_chains, geometry=\"location\")\n",
    "activity_chains.crs = \"EPSG:4326\"  # I assume this is the crs\n",
    "\n",
    "\n",
    "# remove index_right column from activity_chains if it exists\n",
    "if \"index_right\" in activity_chains.columns:\n",
    "    activity_chains = activity_chains.drop(columns=\"index_right\")\n",
    "\n",
    "\n",
    "# Spatial join to identify which polygons each point is in\n",
    "activity_chains = gpd.sjoin(\n",
    "    activity_chains, boundaries[[\"OA21CD\", \"geometry\"]], how=\"left\", predicate=\"within\"\n",
    ")\n",
    "activity_chains = activity_chains.drop(\"index_right\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove location column\n",
    "activity_chains = activity_chains.drop(columns=\"location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_edu = pd.read_pickle(\n",
    "    acbm.root_path / \"data/interim/assigning/activity_chains_education.pkl\"\n",
    ")\n",
    "\n",
    "activity_chains_work = pd.read_pickle(\n",
    "    acbm.root_path / \"data/interim/assigning/activity_chains_work.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_edu.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all activity chains where dact is home\n",
    "activity_chains_home = activity_chains[activity_chains[\"dact\"] == \"home\"]\n",
    "# get all activity chains where dact is not work or education\n",
    "activity_chains_other = activity_chains[\n",
    "    ~activity_chains[\"dact\"].isin([\"work\", \"education\", \"home\"])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ozone and dzone with Na in activity_chains_other. They are incorrect and will be populated later\n",
    "activity_chains_other.loc[:, [\"ozone\", \"dzone\"]] = np.nan\n",
    "activity_chains_other.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(activity_chains.shape[0], \n",
    " activity_chains_edu.shape[0], \n",
    " activity_chains_work.shape[0], \n",
    " activity_chains_home.shape[0], \n",
    " activity_chains_other.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains[\"dact\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add home locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osm data\n",
    "osm_data_gdf = gpd.read_parquet(\n",
    "    acbm.root_path / \"data/external/boundaries/west-yorkshire_epsg_4326.parquet\"\n",
    ")\n",
    "\n",
    "# get rows in osm_data_gdf where activities includes home\n",
    "\n",
    "osm_data_gdf = osm_data_gdf[osm_data_gdf[\"activities\"].str.contains(\"home\")]\n",
    "osm_data_gdf\n",
    "\n",
    "# spatial join to identify which zone each point in osm_data is in\n",
    "osm_data_gdf = gpd.sjoin(\n",
    "    osm_data_gdf, boundaries[[\"OA21CD\", \"geometry\"]], how=\"inner\", predicate=\"within\"\n",
    ")\n",
    "\n",
    "osm_data_gdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate a home location only once per household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep one row per household and select only household and OA21CD columns\n",
    "activity_chains_home_hh = activity_chains_home.drop_duplicates(subset=[\"household\"])\n",
    "activity_chains_home_hh = activity_chains_home_hh[[\"household\", \"dact\", \"OA21CD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_home_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the home location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_neighbors = Queen.from_dataframe(boundaries, ids=\"OA21CD\").neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to a row in activity_chains_ex\n",
    "# TODO: edit function so that we have replacement = false option. We don't want to assign different households to the same home\n",
    "activity_chains_home_hh[[\"activity_id\", \"activity_geom\"]] = activity_chains_home_hh.apply(\n",
    "    lambda row: select_facility(\n",
    "        row=row,\n",
    "        facilities_gdf=osm_data_gdf,\n",
    "        row_destination_zone_col=\"OA21CD\",\n",
    "        row_activity_type_col=\"dact\",\n",
    "        gdf_facility_zone_col=\"OA21CD\",\n",
    "        gdf_facility_type_col=\"activities\",\n",
    "        gdf_sample_col=\"floor_area\",\n",
    "        neighboring_zones=zone_neighbors,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_home_hh.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge home locations back onto activity_chains_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join activity_chains_home_hh onto activity_chains_home based on household column\n",
    "activity_chains_home = activity_chains_home.merge(\n",
    "    activity_chains_home_hh[[\"household\", \"activity_id\", \"activity_geom\"]],\n",
    "    on=\"household\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "activity_chains_home.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace dzone column with OA21CD\n",
    "activity_chains_home[\"dzone\"] = activity_chains_home[\"OA21CD\"]\n",
    "activity_chains_home.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the three dataframes\n",
    "activity_chains_all = pd.concat([activity_chains_edu, \n",
    "                                 activity_chains_work, \n",
    "                                 activity_chains_home,\n",
    "                                 activity_chains_other])\n",
    "# sort by houshold_id, individual_id, and sequence\n",
    "activity_chains_all = activity_chains_all.sort_values(by=[\"household\", \"id\", \"seq\"])\n",
    "activity_chains_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all people who don't start their day at home\n",
    "\n",
    "They will raise an error in PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by id column, and remove all groups where oact is not home in the first row\n",
    "activity_chains_all = activity_chains_all.sort_values(by=[\"household\", \"id\", \"seq\"])\n",
    "\n",
    "print(f'PRE-FILTERING: Number of activities: {activity_chains_all.shape[0]}, number of individuals: {activity_chains_all[\"id\"].nunique()}')\n",
    "total_activities = activity_chains_all.shape[0]\n",
    "\n",
    "activity_chains_all = activity_chains_all.groupby(\"id\").filter(lambda x: x.iloc[0][\"oact\"] == \"home\")\n",
    "\n",
    "print(f'POST-FILTERING: Number of activities: {activity_chains_all.shape[0]}, number of individuals: {activity_chains_all[\"id\"].nunique()}')\n",
    "\n",
    "removed_activities = total_activities - activity_chains_all.shape[0]\n",
    "percentage_removed = (removed_activities / total_activities) * 100\n",
    "print(f'Removed {removed_activities} activities, which is {percentage_removed:.2f}% of the total activities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check modes\n",
    "\n",
    "We can only use modes that we have travel times for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_all[\"mode\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace motorcyle with car\n",
    "activity_chains_all[\"mode\"] = activity_chains_all[\"mode\"].replace(\"motorcycle\", \"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate ozone column for primary activities\n",
    "\n",
    "Our dfs have populated the `dzone`, `activity_id`, and `activity_geom` columns for rows where `dact` matches: [home, work, education]. \n",
    "For each person, we look at rows where the `ozone` is one of [home, work, education], and populate the `ozone`, `origin_id`, `origin_geom` columns for the primary activity with the same value.\n",
    "\n",
    "TODO: rename `activity_id` to `destination_id` and `activity_geom` to `destination_geom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create dictionaries to map each id to their activity_geom, activity_id, and dzone for each activity type\n",
    "activity_types = [\"home\", \"education\", \"work\"]\n",
    "activity_geom_dict = {}\n",
    "activity_id_dict = {}\n",
    "dzone_dict = {}\n",
    "\n",
    "for activity in activity_types:\n",
    "    filtered_df = activity_chains_all[\n",
    "        (activity_chains_all[\"dact\"] == activity) & (activity_chains_all[\"activity_geom\"].notnull())\n",
    "    ]\n",
    "    activity_geom_dict[activity] = filtered_df.set_index(\"id\")[\"activity_geom\"].to_dict()\n",
    "    activity_id_dict[activity] = filtered_df.set_index(\"id\")[\"activity_id\"].to_dict()\n",
    "    dzone_dict[activity] = filtered_df.set_index(\"id\")[\"dzone\"].to_dict()\n",
    "\n",
    "# Step 2: Populate the origin_geom, origin_id, and ozone columns based on the oact value\n",
    "def get_origin_geom(row):\n",
    "    if row[\"oact\"] in activity_geom_dict and row[\"id\"] in activity_geom_dict[row[\"oact\"]]:\n",
    "        return activity_geom_dict[row[\"oact\"]][row[\"id\"]]\n",
    "    return None\n",
    "\n",
    "def get_origin_id(row):\n",
    "    if row[\"oact\"] in activity_id_dict and row[\"id\"] in activity_id_dict[row[\"oact\"]]:\n",
    "        return activity_id_dict[row[\"oact\"]][row[\"id\"]]\n",
    "    return None\n",
    "\n",
    "def get_ozone(row):\n",
    "    if row[\"oact\"] in dzone_dict and row[\"id\"] in dzone_dict[row[\"oact\"]]:\n",
    "        return dzone_dict[row[\"oact\"]][row[\"id\"]]\n",
    "    return np.nan\n",
    "\n",
    "activity_chains_all = activity_chains_all.copy()\n",
    "activity_chains_all[\"origin_geom\"] = activity_chains_all.apply(get_origin_geom, axis=1)\n",
    "activity_chains_all[\"origin_id\"] = activity_chains_all.apply(get_origin_id, axis=1)\n",
    "activity_chains_all[\"ozone\"] = activity_chains_all.apply(get_ozone, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_all = activity_chains_all[[\"id\", \"household\", \"nts_ind_id\", \"nts_hh_id\", \"age_years\", \n",
    "                                           \"oact\", \"dact\", \"TripTotalTime\", \"TripDisIncSW\", \"seq\", \"mode\", \"tst\", \"tet\", \n",
    "                                           \"ozone\", \"dzone\", \"origin_id\", \"origin_geom\", \"activity_id\", \"activity_geom\"]]\n",
    "\n",
    "activity_chains_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAM needs a hzone column. Add it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the DataFrames on 'household_id' from activity_chains and 'house_id' from activity_chains_home_hh\n",
    "activity_chains_all = activity_chains_all.merge(activity_chains_home_hh[['household', 'OA21CD']], on='household', how ='left')\n",
    "\n",
    "# Rename the 'OA21CD' column to 'hzone'\n",
    "activity_chains_all.rename(columns={'OA21CD': 'hzone'}, inplace=True)\n",
    "\n",
    "activity_chains_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add home locations for all rows where oact = \"home\"\n",
    "\n",
    "I was populating home data based on its existence in the dact column. Some entries are outliers which were missed by this logic.\n",
    "\n",
    "See :\n",
    "\n",
    "- activity_chains_all[activity_chains_all[\"id\"] == 808]\n",
    "- activity_chains_all[activity_chains_all[\"id\"] == 1994]\n",
    "\n",
    "This adds home data for oact = home (MESSY: REDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_chains_all[activity_chains_all[\"id\"] == 808]\n",
    "# activity_chains_all[activity_chains_all[\"id\"] == 1994]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns then select necessary ones \n",
    "activity_chains_home_hh_selected = activity_chains_home_hh.rename(columns={\n",
    "    'OA21CD': 'ozone', \n",
    "    'activity_id': 'origin_id', \n",
    "    'activity_geom': 'origin_geom'\n",
    "})[['household', 'ozone', 'origin_id', 'origin_geom']]\n",
    "\n",
    "# Merge activity_chains_all with activity_chains_home_hh_selected on 'household'\n",
    "merged_df = activity_chains_all.merge(activity_chains_home_hh_selected, on='household', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Update only the rows where 'oact' is 'home'\n",
    "home_mask = merged_df['oact'] == 'home'\n",
    "for column in ['ozone', 'origin_id', 'origin_geom']:\n",
    "    merged_df.loc[home_mask, column] = merged_df.loc[home_mask, f'{column}_new']\n",
    "\n",
    "# Drop the temporary columns\n",
    "merged_df.drop(columns=[f'{column}_new' for column in ['ozone', 'origin_id', 'origin_geom']], inplace=True)\n",
    "\n",
    "# Assign merged_df back to activity_chains_all\n",
    "activity_chains_all = merged_df\n",
    "\n",
    "# Print the updated DataFrame\n",
    "activity_chains_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for PAM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = activity_chains_all[['id', 'household', 'age_years']].drop_duplicates(subset=['id'])\n",
    "\n",
    "# rename columns\n",
    "individuals = individuals.rename(columns={\"id\": \"pid\", \"household\": \"hid\"})\n",
    "individuals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Households"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = activity_chains_all[['id', 'household', 'seq', 'hzone', 'ozone', 'dzone', 'dact', 'mode', 'tst', 'tet']]\n",
    "\n",
    "# rename columns\n",
    "trips = trips.rename(columns={\"id\": \"pid\", \"household\": \"hid\", \"dact\": \"purp\"})\n",
    "\n",
    "# Drop NA values in tst and tet columns and convert to int\n",
    "trips = trips.dropna(subset=['tst', 'tet'])\n",
    "trips['tst'] = trips['tst'].astype(int)\n",
    "trips['tet'] = trips['tet'].astype(int)\n",
    "\n",
    "trips.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Nan values in ozone and dzone with \"na\"\n",
    "trips['ozone'] = trips['ozone'].apply(lambda x: None if pd.isna(x) else x)\n",
    "trips['dzone'] = trips['dzone'].apply(lambda x: None if pd.isna(x) else x)\n",
    "trips.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read population \n",
    "\n",
    "tour_based = False assumes all trips start from home - this is ok for matsim \n",
    "see here https://arup-group.github.io/pam/latest/reference/pam/read/diary/#pam.read.diary.load_travel_diary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pam.read.load_travel_diary(\n",
    "    trips = trips,\n",
    "    persons_attributes = individuals,\n",
    "    tour_based = False\n",
    "    #hhs_attributes = None,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activity_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_leg_times(population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices for OD Factory \n",
    "\n",
    "The PAM OD factory function needs the following matrices\n",
    "\n",
    "- travel times (by mode)\n",
    "- travel distances (this appears optional so I will ignore it for now)\n",
    "- od_probs: probability of travelling between each pair of zones (by mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data: Travel time matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_times = pd.read_parquet(\n",
    "    acbm.root_path / \"data/external/travel_times/oa/travel_time_matrix_acbm.parquet\"\n",
    ")\n",
    "\n",
    "travel_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit modes\n",
    "\n",
    "We have travel times for PT by time of day. In discretionary trips, PAM needs the mode column to match the mode labels in ODFactory (see https://github.com/arup-group/pam/blob/main/examples/17_advanced_discretionary_locations.ipynb). We have two options\n",
    "\n",
    "1. TODO: Preferred: Before reading the population into PAM, edit the mode column of the trips table to replace pt with pt_wkday_morning, pt_wkday_evening etc depending on day and time of trip. I dont know if this will work downstream\n",
    "2. Simplify our travel time data. Use the same travel time regardless of time of day, and label as pt (to match with mode column)\n",
    "\n",
    "I will do 2 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows that match specific \"combination\" values\n",
    "\n",
    "modes_to_use = ['car', 'walk', 'cycle', 'pt_wkday_morning']\n",
    "\n",
    "# Filter the DataFrame\n",
    "travel_times = travel_times[travel_times['combination'].isin(modes_to_use)]\n",
    "\n",
    "# Rename specific values in \"combination\" column\n",
    "travel_times['combination'] = travel_times['combination'].replace({\n",
    "    'cycle': 'bike',\n",
    "    'pt_wkday_morning': 'pt'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add OA21CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from_id and to_id to int to match the boundaries data type\n",
    "travel_times = travel_times.astype({\"from_id\": int, \"to_id\": int})\n",
    "\n",
    "# merge travel_times with boundaries\n",
    "travel_times = travel_times.merge(\n",
    "    boundaries[[\"OBJECTID\", \"OA21CD\"]],\n",
    "    left_on=\"from_id\",\n",
    "    right_on=\"OBJECTID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "travel_times = travel_times.drop(columns=\"OBJECTID\")\n",
    "\n",
    "travel_times = travel_times.merge(\n",
    "    boundaries[[\"OBJECTID\", \"OA21CD\"]],\n",
    "    left_on=\"to_id\",\n",
    "    right_on=\"OBJECTID\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_from\", \"_to\"),\n",
    ")\n",
    "travel_times = travel_times.drop(columns=\"OBJECTID\")\n",
    "\n",
    "travel_times.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data: OD probabilities\n",
    "\n",
    "We use the activities_per_zone data to calculate the OD probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_per_zone = pd.read_parquet(\n",
    "    acbm.root_path / \"data/interim/assigning/activities_per_zone.parquet\"\n",
    ")\n",
    "\n",
    "activities_per_zone.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows that don't match primary activities\n",
    "activities_per_zone = activities_per_zone[activities_per_zone[\"activity\"].isin([\"shop\", \"other\", \"medical\", \"visit\"])]\n",
    "\n",
    "# group by zone and get sum of counts and floor_area\n",
    "activities_per_zone = activities_per_zone.groupby(\"OA21CD\").agg({\"counts\": \"sum\", \"floor_area\": \"sum\"}).reset_index()\n",
    "activities_per_zone.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to get floor_area for origin\n",
    "merged_df = travel_times.merge(activities_per_zone, left_on='OA21CD_to', right_on='OA21CD')\n",
    "\n",
    "# Calculate the visit_probability: it is a funciton of floor_area and travel time\n",
    "merged_df['visit_prob'] = np.where(merged_df['travel_time_p50'] != 0, \n",
    "                              round(merged_df['floor_area'] / np.sqrt(merged_df['travel_time_p50'])), \n",
    "                              round(merged_df['floor_area'])\n",
    "                              )\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_od_matrices(\n",
    "        df: pd.DataFrame, \n",
    "        mode_column: str, \n",
    "        value_column: str,\n",
    "        zone_labels: tuple,\n",
    "        fill_value: int,\n",
    "        zone_from: str = 'OA21CD_from',\n",
    "        zone_to: str = 'OA21CD_to',\n",
    "        ) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Create OD matrices for each mode in the DataFrame. This function is uused to create matrices for \n",
    "    - travel times\n",
    "    - od_probs\n",
    "    to be used in discretionary activity selection\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data\n",
    "    mode_column : str\n",
    "        Column name containing the mode of transport\n",
    "    value_column : str\n",
    "        Column name containing the value to be used in the OD matrix\n",
    "    fill_value : int\n",
    "        Value to use when a value for a specific od pair is not available\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing OD matrices for each mode.\n",
    "        Key: str\n",
    "            Mode of transport\n",
    "        Value: np.array\n",
    "            OD matrix\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionaries to hold OD matrices for each combination type\n",
    "    modes = df[mode_column].unique()\n",
    "    od_matrices = {mode: np.full((len(zone_labels), len(zone_labels)), fill_value) for mode in modes}\n",
    "    \n",
    "    # Create a mapping from zone labels to indices\n",
    "    zone_index = {label: idx for idx, label in enumerate(zone_labels)}\n",
    "    \n",
    "    # Vectorized operation to populate OD matrices\n",
    "    from_indices = df[zone_from].map(zone_index)\n",
    "    to_indices = df[zone_to].map(zone_index)\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"Starting mode: {mode}\")\n",
    "        mask = df[mode_column] == mode\n",
    "        values = df[mask][value_column].fillna(fill_value)  # Fill missing values \n",
    "        od_matrices[mode][from_indices[mask], to_indices[mask]] = values\n",
    "        print(f\"Finished mode: {mode}\")\n",
    "    \n",
    "    return od_matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique zone labels. \n",
    "# TODO: get these from boundary/zone layer instead\n",
    "zone_labels = pd.unique(travel_times[['OA21CD_from', 'OA21CD_to']].values.ravel('K'))\n",
    "zone_labels = tuple(zone_labels)\n",
    "print(zone_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create travel time matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_travel_times = create_od_matrices(\n",
    "    df = merged_df, \n",
    "    mode_column = 'combination', \n",
    "    value_column = 'travel_time_p50', \n",
    "    zone_labels = zone_labels,\n",
    "    fill_value = 300,  # replace missing travel times with 6 hours (they are unreachable)\n",
    "    zone_from='OA21CD_from', \n",
    "    zone_to='OA21CD_to'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_travel_times[\"car\"][100:110, 100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create od probs matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_od_probs = create_od_matrices(\n",
    "    df = merged_df, \n",
    "    mode_column = 'combination', \n",
    "    value_column = 'visit_prob', \n",
    "    zone_labels = zone_labels,\n",
    "    # replace missing probabilities with 1. There are no activities so shouldn't be visited\n",
    "    # 1 used instead of 0 to avoid (ValueError: Total of weights must be finite) in weighted sampling \n",
    "    # (https://github.com/arup-group/pam/blob/c8bff760fbf92f93f95ff90e4e2af7bbe107c7e3/src/pam/planner/utils_planner.py#L17)\n",
    "    fill_value = 1,  \n",
    "    zone_from='OA21CD_from', \n",
    "    zone_to='OA21CD_to'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_od_probs[\"walk\"][100:110, 100:110]\n",
    "matrix_od_probs[\"walk\"][0:10, 0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ODMatrix objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_types = travel_times['combination'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_pam_travel_time = [\n",
    "    ODMatrix(\"time\", mode, zone_labels, zone_labels, matrix_travel_times[mode])\n",
    "    for mode in mode_types\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_pam_travel_time[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_pam_od_probs = [\n",
    "    ODMatrix(\"od_probs\", mode, zone_labels, zone_labels, matrix_od_probs[mode])\n",
    "    for mode in mode_types\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ODFactory object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine ODMatrix objects\n",
    "matrices_pam_all = matrices_pam_travel_time + matrices_pam_od_probs\n",
    "matrices_pam_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ODFactory\n",
    "od = ODFactory.from_matrices(matrices = matrices_pam_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretionary activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation of discretionary activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activity_locs(plan):\n",
    "    summary = PrettyTable([\"seq\", \"purpose\", \"location\"])\n",
    "    for seq, act in enumerate(plan.activities):\n",
    "        summary.add_row([seq, act.act, act.location.area])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "plans_iterator = population.plans()\n",
    "all_plans = list(plans_iterator)\n",
    "\n",
    "\n",
    "random_plan = random.choice(all_plans)\n",
    "print_activity_locs(random_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_plan_copy = deepcopy(random_plan)\n",
    "planner = DiscretionaryTrips(plan=random_plan_copy, od=od)\n",
    "planner.update_plan()\n",
    "\n",
    "print_activity_locs(random_plan_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply logic to entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "people_list = list(population.people())\n",
    "for plan in population.plans():\n",
    "    try:\n",
    "        planner = DiscretionaryTrips(plan=plan, od=od)\n",
    "        planner.update_plan()\n",
    "        print(f\"Updated plan for person id {people_list[i][1]}\")\n",
    "    except Exception as e:\n",
    "        # a pam population.people() object has hid, pid, <plan>. We want pid\n",
    "        print(f\"An error occurred with person id {people_list[i][1]}: {e}\")\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_population_plans(population: pam.core.Population, \n",
    "                            od: ODFactory\n",
    "                            ) -> None:\n",
    "    \"\"\"\n",
    "    Update the plans in a population object using the DiscretionaryTrips planner\n",
    "\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    people_list = list(population.people())\n",
    "    for plan in population.plans():\n",
    "        try:\n",
    "            planner = DiscretionaryTrips(plan=plan, od=od)\n",
    "            planner.update_plan()\n",
    "            logger.info(f\"Updated plan for person id {people_list[i][0]}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred with person id {people_list[i][0]}: {e}\")\n",
    "        i += 1\n",
    "\n",
    "update_population_plans(population, od)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.to_csv(population, dir=(\n",
    "    acbm.root_path / \"data/processed/activities_pam\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read activities back and assign point locaitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_pam = pd.read_csv(\n",
    "    acbm.root_path / \"data/processed/activities_pam/activities.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we need\n",
    "activities_pam = activities_pam[['pid', 'hid', 'activity', 'zone']]\n",
    "# select all rows where activity is not home, work, education\n",
    "activities_pam = activities_pam[~activities_pam['activity'].isin(['home', 'work', 'education'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_pam[\"activity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to a row in activity_chains_ex\n",
    "activities_pam[[\"activity_id\", \"activity_geom\"]] = activities_pam.apply(\n",
    "    lambda row: select_facility(\n",
    "        row=row,\n",
    "        facilities_gdf=osm_data_gdf,\n",
    "        row_destination_zone_col=\"zone\",\n",
    "        row_activity_type_col=\"activity\",\n",
    "        gdf_facility_zone_col=\"OA21CD\",\n",
    "        gdf_facility_type_col=\"activities\",\n",
    "        gdf_sample_col=\"floor_area\",\n",
    "        fallback_to_random=True,\n",
    "        neighboring_zones=zone_neighbors,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add locations of secondary activities onto the original activity chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_chains_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
